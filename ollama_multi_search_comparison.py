#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ollama gpt-oss:120b-cloudë¥¼ ì‚¬ìš©í•œ ë‹¤ì¤‘ ê²€ìƒ‰ ì—”ì§„ ë¹„êµ í…ŒìŠ¤íŠ¸
"""

import json
import time
import requests
from typing import Dict, List
from web_search_v2 import WebSearcher as TavilySearcher
from naver_search import NaverSearcher
from duckduckgo_searcher import DuckDuckGoSearcher
from google_search import GoogleSearcher


class OllamaMultiSearchComparison:
    """Ollama LLMì„ ì‚¬ìš©í•œ ë‹¤ì¤‘ ê²€ìƒ‰ ì—”ì§„ ë¹„êµ í´ë˜ìŠ¤"""
    
    def __init__(self, model_name="gpt-oss:120b-cloud", ollama_url="http://localhost:11434"):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_name (str): ì‚¬ìš©í•  Ollama ëª¨ë¸ëª…
            ollama_url (str): Ollama ì„œë²„ URL
        """
        self.model_name = model_name
        self.ollama_url = ollama_url
        
        # ê²€ìƒ‰ ì—”ì§„ë“¤ ì´ˆê¸°í™”
        self.searchers = {
            "Tavily": TavilySearcher(),
            "Naver": NaverSearcher(),
            "DuckDuckGo": DuckDuckGoSearcher(),
            "Google": GoogleSearcher()
        }
        
        self.test_queries = [
            "ëŒ€í•œë¯¼êµ­ì˜ í˜„ì¬ ëŒ€í†µë ¹ì€ ëˆ„êµ¬ì¸ê°€ìš”?",
            "A2Aê°€ AI to ALL ì´ ë§ì•„?"
        ]
    
    def check_ollama_connection(self):
        """Ollama ì„œë²„ ì—°ê²° í™•ì¸"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_names = [model['name'] for model in models]
                if self.model_name in model_names:
                    print(f"âœ… Ollama ì—°ê²° ì„±ê³µ: {self.model_name} ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥")
                    return True
                else:
                    print(f"âŒ ëª¨ë¸ {self.model_name}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {model_names}")
                    return False
            else:
                print(f"âŒ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {response.status_code}")
                return False
        except Exception as e:
            print(f"âŒ Ollama ì„œë²„ ì—°ê²° ì˜¤ë¥˜: {e}")
            return False
    
    def _create_prompt(self, query: str, search_results: List[Dict], engine_name: str) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ LLM í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì •ë¦¬
        search_text = f"ì‚¬ìš©ì ì§ˆë¬¸: {query}\n\n"
        search_text += f"ê²€ìƒ‰ ì—”ì§„: {engine_name}\n"
        search_text += "ì›¹ ê²€ìƒ‰ ê²°ê³¼:\n"
        
        for i, result in enumerate(search_results[:5], 1):  # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
            title = result.get('title', 'ì œëª© ì—†ìŒ')
            content = result.get('content', 'ë‚´ìš© ì—†ìŒ')
            url = result.get('url', 'URL ì—†ìŒ')
            
            search_text += f"\n{i}. ì œëª©: {title}\n"
            search_text += f"   ë‚´ìš©: {content[:500]}...\n"  # ë‚´ìš©ì€ 500ìë¡œ ì œí•œ
            search_text += f"   ì¶œì²˜: {url}\n"
        
        prompt = f"""ë‹¹ì‹ ì€ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

{search_text}

ìœ„ì˜ ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

1. **í•µì‹¬ ë‹µë³€**: ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì´ê³  ëª…í™•í•œ ë‹µë³€
2. **ìƒì„¸ ì„¤ëª…**: ë‹µë³€ì˜ ê·¼ê±°ì™€ ì¶”ê°€ ì •ë³´
3. **ì°¸ê³  ìë£Œ**: ë‹µë³€ì— ì‚¬ìš©ëœ ê²€ìƒ‰ ê²°ê³¼ì˜ ì¶œì²˜

ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ê³ , ê²€ìƒ‰ ê²°ê³¼ì— ê¸°ë°˜í•œ ì‚¬ì‹¤ì ì´ê³  ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”. ë§Œì•½ ê²€ìƒ‰ ê²°ê³¼ë¡œ ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ì—†ë‹¤ë©´ ì†”ì§í•˜ê²Œ ê·¸ë ‡ê²Œ ë§í•´ì£¼ì„¸ìš”."""

        return prompt
    
    def _query_ollama(self, prompt: str) -> str:
        """Ollama APIë¥¼ í†µí•´ LLMì— ì§ˆì˜"""
        try:
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "num_predict": 2000
                }
            }
            
            headers = {
                "Content-Type": "application/json"
            }
            
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                headers=headers,
                timeout=120
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
            else:
                return f"LLM ìš”ì²­ ì‹¤íŒ¨: {response.status_code} - {response.text}"
                
        except Exception as e:
            return f"LLM ìš”ì²­ ì˜¤ë¥˜: {str(e)}"
    
    def test_single_query_with_all_engines(self, query: str) -> Dict:
        """ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•´ ëª¨ë“  ê²€ìƒ‰ ì—”ì§„ìœ¼ë¡œ ê²€ìƒ‰í•˜ê³  LLM ë‹µë³€ ìƒì„±"""
        results = {
            "query": query,
            "engines": {},
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
        }
        
        for engine_name, searcher in self.searchers.items():
            print(f"ğŸ” {engine_name}ì—ì„œ '{query}' ê²€ìƒ‰ ì¤‘...")
            
            try:
                # 1. ì›¹ ê²€ìƒ‰ ìˆ˜í–‰
                search_result = searcher.search(query)
                
                if search_result.get("error", False):
                    results["engines"][engine_name] = {
                        "search_success": False,
                        "search_error": search_result.get("message", ""),
                        "llm_response": None,
                        "search_results": []
                    }
                    print(f"âŒ {engine_name}: ê²€ìƒ‰ ì‹¤íŒ¨ - {search_result.get('message', '')}")
                    continue
                
                search_data = search_result.get("results", [])
                if not search_data:
                    results["engines"][engine_name] = {
                        "search_success": True,
                        "search_error": "",
                        "llm_response": "ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                        "search_results": []
                    }
                    print(f"âš ï¸ {engine_name}: ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                    continue
                
                print(f"âœ… {engine_name}: {len(search_data)}ê°œ ê²°ê³¼ ë°œê²¬")
                
                # 2. LLM í”„ë¡¬í”„íŠ¸ ìƒì„±
                prompt = self._create_prompt(query, search_data, engine_name)
                
                # 3. LLMìœ¼ë¡œ ë¶„ì„ ìš”ì²­
                print(f"ğŸ¤– {engine_name} ê²°ê³¼ë¡œ LLM ë¶„ì„ ì¤‘...")
                llm_response = self._query_ollama(prompt)
                
                results["engines"][engine_name] = {
                    "search_success": True,
                    "search_error": "",
                    "llm_response": llm_response,
                    "search_results": search_data[:3],  # ìƒìœ„ 3ê°œë§Œ ì €ì¥
                    "result_count": len(search_data)
                }
                
                print(f"âœ… {engine_name}: LLM ë¶„ì„ ì™„ë£Œ")
                
            except Exception as e:
                results["engines"][engine_name] = {
                    "search_success": False,
                    "search_error": str(e),
                    "llm_response": None,
                    "search_results": []
                }
                print(f"âŒ {engine_name}: ì˜¤ë¥˜ - {str(e)}")
            
            # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ
            time.sleep(2)
        
        return results
    
    def run_all_tests(self) -> List[Dict]:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        all_results = []
        
        print("=== Ollama ë‹¤ì¤‘ ê²€ìƒ‰ ì—”ì§„ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===\n")
        
        # Ollama ì—°ê²° í™•ì¸
        if not self.check_ollama_connection():
            print("Ollama ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return []
        
        for i, query in enumerate(self.test_queries, 1):
            print(f"--- í…ŒìŠ¤íŠ¸ {i}: '{query}' ---")
            result = self.test_single_query_with_all_engines(query)
            all_results.append(result)
            print()
        
        return all_results
    
    def save_results(self, results: List[Dict], filename: str = "ollama_multi_search_results.json"):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def generate_comparison_report(self, results: List[Dict], filename: str = "ollama_search_engine_comparison.md"):
        """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ë¹„êµ ë³´ê³ ì„œ ìƒì„±"""
        content = [
            "# Ollama gpt-oss:120b-cloud ë‹¤ì¤‘ ê²€ìƒ‰ ì—”ì§„ ë¹„êµ ë¶„ì„",
            "",
            "## í…ŒìŠ¤íŠ¸ í™˜ê²½",
            f"- **LLM ëª¨ë¸**: {self.model_name}",
            "- **ê²€ìƒ‰ ì—”ì§„**: Tavily, ë„¤ì´ë²„, DuckDuckGo, Google",
            f"- **í…ŒìŠ¤íŠ¸ ì¼ì‹œ**: {time.strftime('%Y-%m-%d %H:%M:%S')}",
            f"- **í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìˆ˜**: {len(self.test_queries)}ê°œ",
            "",
            "## í…ŒìŠ¤íŠ¸ ê²°ê³¼",
            ""
        ]
        
        for i, result in enumerate(results, 1):
            content.extend([
                f"### í…ŒìŠ¤íŠ¸ {i}: {result['query']}",
                "",
                f"**í…ŒìŠ¤íŠ¸ ì‹œê°„**: {result['timestamp']}",
                "",
                "| ê²€ìƒ‰ ì—”ì§„ | ê²€ìƒ‰ ì„±ê³µ | ê²°ê³¼ ê°œìˆ˜ | LLM ì‘ë‹µ |",
                "|:---|:---:|:---:|:---:|"
            ])
            
            for engine_name, engine_result in result["engines"].items():
                search_success = "âœ…" if engine_result["search_success"] else "âŒ"
                result_count = engine_result.get("result_count", 0)
                llm_available = "âœ…" if engine_result["llm_response"] else "âŒ"
                content.append(f"| {engine_name} | {search_success} | {result_count} | {llm_available} |")
            
            content.extend(["", "#### ìƒì„¸ ë¶„ì„", ""])
            
            for engine_name, engine_result in result["engines"].items():
                if engine_result["search_success"] and engine_result["llm_response"]:
                    content.extend([
                        f"**{engine_name}**",
                        "",
                        "**ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½**:",
                        ""
                    ])
                    
                    for j, item in enumerate(engine_result["search_results"], 1):
                        content.extend([
                            f"{j}. **{item.get('title', 'ì œëª© ì—†ìŒ')}**",
                            f"   - URL: {item.get('url', 'URL ì—†ìŒ')}",
                            f"   - ë‚´ìš©: {item.get('content', 'ë‚´ìš© ì—†ìŒ')[:100]}...",
                            ""
                        ])
                    
                    content.extend([
                        "**LLM ë‹µë³€**:",
                        "",
                        f"```",
                        engine_result["llm_response"],
                        f"```",
                        ""
                    ])
                else:
                    error_msg = engine_result.get("search_error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
                    content.extend([
                        f"**{engine_name}**: ê²€ìƒ‰ ì‹¤íŒ¨",
                        f"- ì˜¤ë¥˜: {error_msg}",
                        ""
                    ])
            
            content.append("---")
            content.append("")
        
        # ìš”ì•½ í†µê³„
        content.extend([
            "## ìš”ì•½ í†µê³„",
            "",
            "| ê²€ìƒ‰ ì—”ì§„ | ì´ í…ŒìŠ¤íŠ¸ | ê²€ìƒ‰ ì„±ê³µ | LLM ì‘ë‹µ ì„±ê³µ | ì„±ê³µë¥  |",
            "|:---|:---:|:---:|:---:|:---:|"
        ])
        
        engine_stats = {}
        for result in results:
            for engine_name, engine_result in result["engines"].items():
                if engine_name not in engine_stats:
                    engine_stats[engine_name] = {"total": 0, "search_success": 0, "llm_success": 0}
                
                engine_stats[engine_name]["total"] += 1
                if engine_result["search_success"]:
                    engine_stats[engine_name]["search_success"] += 1
                if engine_result["llm_response"]:
                    engine_stats[engine_name]["llm_success"] += 1
        
        for engine_name, stats in engine_stats.items():
            success_rate = (stats["llm_success"] / stats["total"]) * 100
            content.append(f"| {engine_name} | {stats['total']} | {stats['search_success']} | {stats['llm_success']} | {success_rate:.1f}% |")
        
        content.extend([
            "",
            "## ê²°ë¡ ",
            "",
            "ì´ í…ŒìŠ¤íŠ¸ë¥¼ í†µí•´ ê° ê²€ìƒ‰ ì—”ì§„ì˜ ê²€ìƒ‰ í’ˆì§ˆê³¼ LLMì„ í†µí•œ ë‹µë³€ ìƒì„± ëŠ¥ë ¥ì„ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "ëª¨ë“  ê²€ìƒ‰ ì—”ì§„ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ë©°, gpt-oss:120b-cloud ëª¨ë¸ì„ í†µí•´ ì¼ê´€ëœ ë‹µë³€ í˜•ì‹ì„ ì œê³µí•©ë‹ˆë‹¤."
        ])
        
        with open(filename, 'w', encoding='utf-8') as f:
            f.write('\n'.join(content))
        
        print(f"ğŸ“„ ë¹„êµ ë³´ê³ ì„œê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    tester = OllamaMultiSearchComparison()
    
    # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = tester.run_all_tests()
    
    if results:
        # ê²°ê³¼ ì €ì¥
        tester.save_results(results)
        tester.generate_comparison_report(results)
        
        print("\n=== í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
        print("ğŸ“Š JSON ê²°ê³¼: ollama_multi_search_results.json")
        print("ğŸ“„ ë¹„êµ ë³´ê³ ì„œ: ollama_search_engine_comparison.md")
    else:
        print("í…ŒìŠ¤íŠ¸ ì‹¤í–‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()
